{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with PCA\n",
    "\n",
    "Given $X \\in R^{D \\times N}$ (a dataset)\n",
    "* $D$: The dimension of each observation\n",
    "* $N$: The number of observations.\n",
    "\n",
    "PCA builds on SVD of the covariance matrix of a dataset $X$. The covariance matrix is a symmetrical matrix of dimension $D$ defining the features' covariance, $\\Sigma_X = cov[X_i, X_j]$ for all features. Since $\\Sigma_X$ is symmetrical of dimension $R^{D\\times D}$ the resulting decomposition $U D V^T$ has the property that $U = V$ and $U,V \\in R^{D \\times D}$.\n",
    "\n",
    "Goal: Reduce dimension $D$ to dimension $K$, s.t. $K << D$\n",
    "* I.e. transform $X \\rightarrow \\tilde{X} \\in R^{K \\times N}$\n",
    "\n",
    "## PCA (Principal Component Analysis)\n",
    "\n",
    "### PCA Procedure\n",
    "#### (1) Compute empirical mean observation\n",
    "Compute the mean along the rows: $\\bar{x} = \\frac{1}{N} \\sum_i^N x_i$, $\\bar{x} \\in R^D$\n",
    "\n",
    "#### (2) Center dataset \n",
    "Center wrt. the empirical mean, by subtracting the mean observation: $\\bar{X} = X - \\bar{x}$\n",
    "\n",
    "#### (3) Compute the covariance matrix\n",
    "Covariance matrix of $X$: $\\Sigma_X = \\frac{1}{N} \\sum_i^N (x_i - \\bar{x}) (x_i - \\bar{x})^T = \\frac{1}{N} \\bar{X} \\bar{X}^T$\n",
    "\n",
    "#### (4) Perform EV decomposition \n",
    "Decompose the the cov. matrix : $\\Sigma_X = U \\Lambda U^T$ (see chapter 1, Eigen-decomposition)\n",
    "* $U \\in R^{D\\times D}$, D is the outer dimension\n",
    "* $\\Lambda \\in R^{D \\times D}$ diagonal matrix (since cov matrix is symmetrical of dimension $D \\times D$)\n",
    "\n",
    "It then holds that $diag(\\Lambda) = \\sigma_i(\\Sigma_X)$ for $i=1...D$ in descending order.\n",
    "\n",
    "$U \\in R^{D\\times K}$\n",
    "Select the first $K < D$ for which a substantial amount of data is preserved, for this it can help inspecting the explained variance: $var = \\frac{\\sum_i^K \\sigma_i^2}{\\sum_j^D \\sigma_j^2}$. A good choice for $K$ will preserve more than 90% of data, while still being a much smaller than D.\n",
    "\n",
    "The $K$ first eigenvectors are then found as the $K$ first columns in $U$, that is, $U_K = [u_1 ... u_K]$ and $\\sigma = [\\sigma_1 ... \\sigma_K]$\n",
    "    \n",
    "#### (5) Compressing the Dataset\n",
    "Downproject centered dataset to new basis: $\\bar{Z} = U_K^T \\bar{X}$\n",
    "\n",
    "#### (6) Reconstructing the Dataset \n",
    "To reconstruct dataset: \n",
    "* Up-project to original basis of dimension $D$: $\\tilde{\\bar{X}}_K = U_K \\bar{Z} = U_K^T U_K \\bar{X}$\n",
    "    * Here, we used the orthonormality of $U$, i.e. $U U^T = I$\n",
    "* Undo centering by adding the mean observation $\\bar{x}$ once again: $\\tilde{X}_K = \\tilde{\\bar{X}}_K + \\bar{x}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
