{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tools for CIL\n",
    "## Matrix-vector basis\n",
    "* Symmetric matrix: $A = A^T$\n",
    "* Orthogonal matrix: $A^{-1} = A^T$ i.e. $A^T A = A^{-1}A = I$ and $det(I) = 1$\n",
    "* Transposed matrix: $(A^T)^{-1} = (A^{-1})^{T}$\n",
    "* Inner Prod: $\\langle x,y \\rangle =\\| x\\|_2 \\cdot \\| y\\| \\cdot cos(\\theta) $, if $y$ is a unit vector then the inner product projects $x$ onto $y$.\n",
    "    * $\\langle x,y \\rangle = x^T y = \\sum_i^N x_i y_i$\n",
    "    * $\\langle x+y, x+y \\rangle = \\langle x, x \\rangle + \\langle y, y \\rangle + 2 \\langle x, y \\rangle$\n",
    "    * $\\langle x-y, x-y \\rangle = \\langle x, x \\rangle + \\langle y, y \\rangle - 2 \\langle x, y \\rangle$\n",
    "    * $\\langle x, y+z \\rangle = \\langle x,y \\rangle + \\langle x,z \\rangle$\n",
    "    * $\\langle x+z, y \\rangle = \\langle x,y \\rangle + \\langle z,y \\rangle$\n",
    "* Outer product: $X = u v^T$ and $X_{i,j} = u_i v_j$\n",
    "* Orthonormal basis: Set of vectors in an $N$ dimensional space for which the basis vectors fulfill:\n",
    "    * Unit vectors (length = 1)\n",
    "    * Together the vectors have an inner product of zero, i.e. the vectors are orthogonal\n",
    "    * Ex for basis for $R^3$: $\\{e_1, e_2, e_3\\} = \\{(0,0,1),(0,1,0), (1,0,0)\\}$\n",
    "        * Being a basis for $R^3$ means that every vector $v \\in R^3$ can be written as a sum of the 3 vectors scaled: $v = e_1 \\cdot x +  e_2 \\cdot y +  e_3 \\cdot z$\n",
    "* Gram-Schmidt orthonormal basis algorithm: Finds an orthonormal basis $u=u_1 ... u_k$ given linearly independent set $v = v_1 ... v_k$ where:\n",
    "    * $u_1 = v_1$\n",
    "    * $u_2 = v_2 - \\frac{\\langle v_2, u_1 \\rangle}{\\langle u_1, u_1 \\rangle}$\n",
    "    * $u_3 = v_3 - \\frac{\\langle v_3, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} - \\frac{\\langle v_3, u_2 \\rangle}{\\langle u_2, u_2 \\rangle} $\n",
    "    * ...\n",
    "    * $u_k = v_k - \\sum_i^{k-1} \\frac{\\langle v_k, u_i \\rangle}{\\langle u_i, u_i \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms \n",
    "### Vector norms\n",
    "* Zero norm: $\\| x\\|_0$ is the number of non-zero elements in $x$\n",
    "    * Formally $|\\{i| x_i \\neq 0\\}|$\n",
    "* P-norm: $\\| x\\|_p = (\\sum_i^N |x_i|^p) ^{\\frac{1}{p}}$\n",
    "    * Ex Euclidean norm: $\\| x\\|_2 = (\\sum_i^N x_i^2) ^{\\frac{1}{2}}$\n",
    "    * Ex one norm $\\| x\\|_1 = (\\sum_i^N |x_i|)$ \n",
    "    \n",
    "### Matrix norms\n",
    "Given $M \\in R^{m\\times n}$, the i'th eigenvalue of $X$ is denoted $\\sigma_i$ or $\\sigma_i(X)$\n",
    "* Fröbenius: $\\| X\\|_F = (\\sum_i^m \\sum_j^n X_{ij}^2) ^{\\frac{1}{2}} = (\\sum_i^{min(m,n)} \\sigma_i^2) ^{\\frac{1}{2}}$\n",
    "* 1-Norm: $\\| X\\|_1 = (\\sum_{i,j}^{m,n} |x_{i,j}|)$\n",
    "* Euclidean norm: $\\| X\\|_2 = \\sigma_{max}(X)$\n",
    "* Spectral norm (p-norm): $\\| X\\|_p = max_{v \\neq 0} \\frac{ \\| Xv\\|_p }{\\| v\\|_p}$\n",
    "* Nuclear norm (star norm): $\\| X\\|_* = \\sum_i^{min(m,n)} \\sigma_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives\n",
    "\n",
    "### Vectors\n",
    "* $\\frac{\\partial}{\\partial x} (b^T x) = \\frac{\\partial}{\\partial x} (x^T b) = b$\n",
    "* $\\frac{\\partial}{\\partial x} (x^T x) = \\frac{\\partial}{\\partial x} (\\| x\\|_2^2)= 2x$\n",
    "* $\\frac{\\partial}{\\partial x} (x^T Ax) = (A^T A) x$ and if $A$ is symm then $=2Ax$\n",
    "* $\\frac{\\partial}{\\partial x} (b^T Ax) = A^T b$\n",
    "* $\\frac{\\partial}{\\partial x} (\\| x-b\\|_2) = \\frac{x-b}{\\|x-b\\|_2}$\n",
    "\n",
    "### Matrices\n",
    "* $\\frac{\\partial}{\\partial X} (c^T Xb) = bc^T$\n",
    "* $\\frac{\\partial}{\\partial X} (\\| X\\|_F^2) = 2X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and eigenvectors\n",
    "* $Ax = \\lambda x$\n",
    "* $A \\in R^{N\\times N}$: square matrix, $x$: column vector, $\\lambda$: scalar\n",
    "\n",
    "### Find eigenvalues\n",
    "The EV problem: Given a matrix $A$ solve the characteristic equation $\\lambda$ s.t. $det(A - \\lambda I) = 0$ which will result in some high degree polynomial, the eigenvalues are then the roots of this polynomial.\n",
    "\n",
    "### Find eigenvectors\n",
    "For each eigenvalue $\\lambda_i$ it holds that $A-\\lambda I)x_i = 0$, $x_i, \\lambda_i$ being the i'th eigenvector, eigenvalue pair. This is a linear system and can be solved by Gaussian elimination.\n",
    "\n",
    "Eigenvectors are not normalized to unit vectors, which is often desired - to fix this perform the following operation $\\tilde{x} = \\frac{x}{\\|x\\|_2}$\n",
    "\n",
    "### Eigen-decomposition\n",
    "* $A$ can be decomposed as $A = Q \\Lambda Q^T$ where $Q$ is an orthogonal matrix ($QQ^T = I$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory\n",
    "* Joint probability of variables $X$ and $Y$: $P(x) := Pr[X = x] := \\sum_{y \\in Y} p(x,y)$\n",
    "* Coniditional probability: $P(x|y) := Pr[X = x | Y = y] := \\frac{p(x,y)}{p(y)}$ where $P(y) > 0$\n",
    "* Necessary property of probability density: $\\forall y\\in Y: \\sum_{x \\in X} p(x|y) = 1$\n",
    "* Marginal probability, chain rule: $p(x,y) = p(x|y) p(y)$\n",
    "* Bayes Theorem using chain rule and conditional probability: $p(x|y) = \\frac{p(y|x) p(x)}{p(y)}$\n",
    "* Independence between stochastic variables: $p(y|x) = p(y)$ then $p(x|y) = p(x)$\n",
    "* Probability of a sequencee of IID obs: $p(x_1, x_2 ... x_N) = \\Pi_i^N p(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "![](svd.png \"SVD illustration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above illustration, and denoting $Sigma$ as $D$ instead, SVD builds on the following:\n",
    "\n",
    "Given a matrix (ex dataset) $X \\in R^{N \\times M}$, $X$ can be decomposed as\n",
    "* $X = U D V^T$\n",
    "\n",
    "* $U^TU = V^T V = I$\n",
    "* Columns of $U$: Eigenvectors of $X X^T$\n",
    "* Columns of $V$: Eigenvectors of $X^T X$\n",
    "* Diag(D): $\\sqrt{\\sigma(X X^T)}$ (also called **singular values**)\n",
    "    * Note: $\\sigma(X X^T) = \\sigma(X^T X)$\n",
    "* $U \\in R^{N\\times N}, D \\in R^{N \\times M}, V \\in R^{M\\times M}$\n",
    "    * We do not require $N = M$, but if $N = M$ then $ U = V$ and $U, D, V \\in R^{N \\times N}$\n",
    "    \n",
    "### The SVD procedure\n",
    "\n",
    "#### (1) Compute Eigenvalues\n",
    "Compute the eigenvalues $\\lambda = [\\lambda_1 ... \\lambda_K]$ for $X X^T$, the singular values are then $\\sigma_i(X) = \\sqrt{\\lambda_i(X)}$.\n",
    "\n",
    "Find the eigenvalues with the characteristic equation: $det(X - \\lambda I) = 0$\n",
    "\n",
    "#### (2) Compute U (Eigenvectors)\n",
    "Solve the linear system: $(X^T X) v_i = \\lambda_i v_i $ for all $\\lambda_i$. The vector $v_i$ is then the eigenvector for eigenvalue $\\lambda_i$.\n",
    "* On matrix form: $(X^T X) V = (D^T D) V$\n",
    "* $D^T D$ being the diagonal matrix containing eigenvalues for $X^T X$.\n",
    "The right singular vectors $V = [v_1 ... v_K]$ are now identified.\n",
    "\n",
    "#### (3) Compute V\n",
    "Solve for $X v_i = \\sigma_i u_i$, or on matrix form $X V = D U$ where $U = [u_1 ... u_K]$ are the left singular vectors. Concretely, this is solved as:\n",
    "* $D^{-1} X V = U D D^{-1} =  X V D^{-1}= U$\n",
    "\n",
    "#### (3) Reconstruction\n",
    "From $X V D^{-1}= U$ let us reconstruct $X$:\n",
    "* $X V D^{-1} D = U D$ so now $X V = U D$\n",
    "* $X V V^T = U D V^T$, and since $V V^T = I$, this means means $X = U D V^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with PCA\n",
    "\n",
    "Given $X \\in R^{D \\times N}$ (a dataset)\n",
    "* $D$: The dimension of each observation\n",
    "* $N$: The number of observations.\n",
    "\n",
    "PCA builds on SVD of the covariance matrix of a dataset $X$. The covariance matrix is a symmetrical matrix of dimension $D$ defining the features' covariance, $\\Sigma_X = cov[X_i, X_j]$ for all features. Since $\\Sigma_X$ is symmetrical of dimension $R^{D\\times D}$ the resulting decomposition $U D V^T$ has the property that $U = V$ and $U,V \\in R^{D \\times D}$.\n",
    "\n",
    "Goal: Reduce dimension $D$ to dimension $K$, s.t. $K << D$\n",
    "* I.e. transform $X \\rightarrow \\tilde{X} \\in R^{K \\times N}$\n",
    "\n",
    "## PCA (Principal Component Analysis)\n",
    "\n",
    "### PCA Procedure\n",
    "#### (1) Compute empirical mean observation\n",
    "Compute the mean along the rows: $\\bar{x} = \\frac{1}{N} \\sum_i^N x_i$, $\\bar{x} \\in R^D$\n",
    "\n",
    "#### (2) Center dataset \n",
    "Center wrt. the empirical mean, by subtracting the mean observation: $\\bar{X} = X - \\bar{x}$\n",
    "\n",
    "#### (3) Compute the covariance matrix\n",
    "Covariance matrix of $X$: $\\Sigma_X = \\frac{1}{N} \\sum_i^N (x_i - \\bar{x}) (x_i - \\bar{x})^T = \\frac{1}{N} \\bar{X} \\bar{X}^T$\n",
    "\n",
    "#### (4) Perform EV decomposition \n",
    "Decompose the the cov. matrix : $\\Sigma_X = U \\Lambda U^T$ (see chapter 1, Eigen-decomposition)\n",
    "* $U \\in R^{D\\times D}$, D is the outer dimension\n",
    "* $\\Lambda \\in R^{D \\times D}$ diagonal matrix (since cov matrix is symmetrical of dimension $D \\times D$)\n",
    "\n",
    "It then holds that $diag(\\Lambda) = \\sigma_i(\\Sigma_X)$ for $i=1...D$ in descending order.\n",
    "\n",
    "$U \\in R^{D\\times K}$\n",
    "Select the first $K < D$ for which a substantial amount of data is preserved, for this it can help inspecting the explained variance: $var = \\frac{\\sum_i^K \\sigma_i^2}{\\sum_j^D \\sigma_j^2}$. A good choice for $K$ will preserve more than 90% of data, while still being a much smaller than D.\n",
    "\n",
    "The $K$ first eigenvectors are then found as the $K$ first columns in $U$, that is, $U_K = [u_1 ... u_K]$ and $\\sigma = [\\sigma_1 ... \\sigma_K]$\n",
    "    \n",
    "#### (5) Compressing the Dataset\n",
    "Downproject centered dataset to new basis: $\\bar{Z} = U_K^T \\bar{X}$\n",
    "\n",
    "#### (6) Reconstructing the Dataset \n",
    "To reconstruct dataset: \n",
    "* Up-project to original basis of dimension $D$: $\\tilde{\\bar{X}} = U_K \\bar{Z} = U_K^T U_K \\bar{X} = I \\bar{X} = \\bar{X}$\n",
    "    * Here, we used the orthonormality of $U$, i.e. $U U^T = I$\n",
    "* Undo centering by adding the mean observation $\\bar{x}$ once again: $\\tilde{X} = \\tilde{\\bar{X}} + \\bar{x}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust PCA\n",
    "* Corrupted points\n",
    "\n",
    "Convex optimization\n",
    "* Convex sets\n",
    "* Rank of matrix\n",
    "* Norms and zero norm\n",
    "\n",
    "Lagrangians\n",
    "* Lagrange duality\n",
    "* Lagrangian Dual function\n",
    "\n",
    "Gradient descent\n",
    "* Condition for learning rate to guarantee convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
