{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "## Lexical semtantics\n",
    "Idea: Words rarely carry meaning on their own\n",
    "\n",
    "_You shall know the meaning of a word by the company it keeps_\n",
    "\n",
    "I.e. look at which contexts words are used in, to derive meaning.\n",
    "\n",
    "\n",
    "## Semantic Representation\n",
    "Given examples of words usages (corpora of text), learn word representations which capture the semantic meanings of the words.\n",
    "\n",
    "### Vector representations\n",
    "Similarilty measures such as dot product (angle) between vectors should relate to word meanings.\n",
    "\n",
    "### Context Model Likelihood\n",
    "A probabilistic model for predicting the next words in some context, given current word $w^t$\n",
    "$$ L(\\theta) = \\sum_{t=1}^T \\sum_{i\\in C} \\text{log} \\ p_\\theta(w^{t+i} | w^t)$$\n",
    "\n",
    "Offsets in context $C = [-R,...,-1, 1, ..., R]$ \n",
    "\n",
    "Perform MLE on $L_\\theta$ such that the model $p_\\theta$ can be parametrized. Optimal model is the model assigning the highest probability to _observed context_, i.e. this is a language modelling task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Vector Models\n",
    "\n",
    "### Basic model\n",
    "Embed word $w$ as vector $x_w \\in R^d$ in some $d$-dimensional space, given words in the vocabulary $V$.\n",
    "\n",
    "Log-bilinear model\n",
    "$$\\text{log} \\ p_\\theta (w| w') = \\langle x_w, x_{w'} \\rangle$$\n",
    "\n",
    "Main effects of using inner product:\n",
    "* When inner product high, i.e. small angle between vectors, the likelihood $p_\\theta (w | w')$ goes up\n",
    "\n",
    "Normalizing the probability distribution with softmax:\n",
    "\n",
    "$$p_\\theta (w | w') = \\frac{exp(\\langle x_w, x_{w'} \\rangle)}{\\sum_{v \\in V} exp(\\langle x_v, x_{w'} \\rangle)}$$\n",
    "\n",
    "### Skip-Gram Model\n",
    "Use LLH of the bilinear model:\n",
    "\n",
    "$$ L(\\theta) = \\sum_{t=1}^T \\sum_{i\\in C} \\text{log} \\ p_\\theta (w^{t+i}| w^t)$$\n",
    "\n",
    "$$= \\sum_{t=1}^T \\sum_{i\\in C} \\Bigg[ \\langle x_{w^{t+i}}, x_{w^{t}} \\rangle - \\text{log} \\sum_{v \\in V} exp(\\langle x_v, x_{w^t} \\rangle) \\Bigg]$$\n",
    "\n",
    "Model params: Word embeddings $x_w \\in R^d$ for every $w \\in V$, i.e. embedding matrix $X \\in R^{d}{|V|}$\n",
    "\n",
    "#### Negative sampling\n",
    "In order to train problem as logistic regression we need negative examples. Therefore it is necessary to somehow generate negative examples for context words.\n",
    "\n",
    "We sample random context words $w_j ~ P(w_j)^{3/4}$\n",
    "* Here the exponent dampens frequent words, so they are accounted for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "#### Co-occurence matrix\n",
    "Let the co-oc matrix be defined as\n",
    "$$N \\in \\mathbb{N}^{|V|\\times |V|}$$ \n",
    "\n",
    "\n",
    "Where $n_{ij}$ is the number of times word $w_i$ occured in the context of $w_j$. The co-oc matrix is very sparse, since many words do not appear together.\n",
    "\n",
    "\n",
    "#### Objective\n",
    "The glove objective uses a weighting function for frequent words, which clips their frequency by some number $n_{max}$ and also filters out infrequent words by letting their weight become incredibly small:\n",
    "\n",
    "$$f(n_{ij}) = \\text{min} \\Bigg\\{ 1, \\bigg(\\frac{n_{ij}}{n_{max}}^\\alpha\\bigg)\\Bigg\\}, \\quad \\alpha = 3/4$$\n",
    "\n",
    "$$H(\\theta; N) = \\sum_{(i,j)} f(n_{ij}) \\Bigg( \\text{log} n_{ij} - \\text{log} \\tilde{p}_\\theta (w_i | w_j) \\Bigg)$$\n",
    "\n",
    "Where $\\tilde{p}_\\theta$ is an unnormalized distribution $\\tilde{p}_\\theta = \\langle x_i, y_j \\rangle$\n",
    "* No computation of normalization needed (difficult becasuse it is a sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe as Matrix factorization\n",
    "Let $M = \\text{log} \\ N$, i.e. $m_{ij} = \\text{log} \\ n_{ij}$\n",
    "\n",
    "GloVe can then be solved as the following matrix factorization problem:\n",
    "\n",
    "$$min_{X,Y} \\|M - X^T Y\\|_F^2$$\n",
    "\n",
    "With $f(n_{ij})$ as binary mask, $1$ if $n_{ij} > 0$, otherwise $0$.\n",
    "\n",
    "Concretely:\n",
    "\n",
    "$$min_{X,Y} \\sum_{i, j: n_{ij} > 0} (m_{ij} - (X^T Y)_{ij})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Problem\n",
    "Can't just derive global minimum, instead just SGD, optimize in an alternating fashion:\n",
    "\n",
    "$$x_i \\leftarrow x_i + 2 \\eta \\cdot f(n_{ij}) (\\log n_{ij} - \\langle x_i, y_j\\rangle) y_j$$\n",
    "\n",
    "$$y_j \\leftarrow y_j + 2 \\eta \\cdot f(n_{ij}) (\\log n_{ij} - \\langle x_i, y_j\\rangle) x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicussion\n",
    "* Relatedness and similarity captured well\n",
    "* Antonyms not well captured, usually words will be close to each other even though they mean the opposite, because they are often used in the same context\n",
    "    * Ex \"cheap\" vs \"expensive\"\n",
    "    \n",
    "Sentence/document-level embeddings\n",
    "* Aggregation of word vectors, word order will be lost\n",
    "* Instead use convolutional/recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
