{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tools for CIL\n",
    "## Matrix-vector basis\n",
    "* Symmetric matrix: $A = A^T$\n",
    "* Orthogonal matrix: $A^{-1} = A^T$ i.e. $A^T A = A^{-1}A = I$ and $det(I) = 1$\n",
    "* Transposed matrix: $(A^T)^{-1} = (A^{-1})^{T}$\n",
    "* Inner Prod: $\\langle x,y \\rangle =\\| x\\|_2 \\cdot \\| y\\| \\cdot cos(\\theta) $, if $y$ is a unit vector then the inner product projects $x$ onto $y$.\n",
    "    * $\\langle x,y \\rangle = x^T y = \\sum_i^N x_i y_i$\n",
    "    * $\\langle x+y, x+y \\rangle = \\langle x, x \\rangle + \\langle y, y \\rangle + 2 \\langle x, y \\rangle$\n",
    "    * $\\langle x-y, x-y \\rangle = \\langle x, x \\rangle + \\langle y, y \\rangle - 2 \\langle x, y \\rangle$\n",
    "    * $\\langle x, y+z \\rangle = \\langle x,y \\rangle + \\langle x,z \\rangle$\n",
    "    * $\\langle x+z, y \\rangle = \\langle x,y \\rangle + \\langle z,y \\rangle$\n",
    "* Outer product: $X = u v^T$ and $X_{i,j} = u_i v_j$\n",
    "* Orthonormal basis: Set of vectors in an $N$ dimensional space for which the basis vectors fulfill:\n",
    "    * Unit vectors (length = 1)\n",
    "    * Together the vectors have an inner product of zero, i.e. the vectors are orthogonal\n",
    "    * Ex for basis for $R^3$: $\\{e_1, e_2, e_3\\} = \\{(0,0,1),(0,1,0), (1,0,0)\\}$\n",
    "        * Being a basis for $R^3$ means that every vector $v \\in R^3$ can be written as a sum of the 3 vectors scaled: $v = e_1 \\cdot x +  e_2 \\cdot y +  e_3 \\cdot z$\n",
    "* Gram-Schmidt orthonormal basis algorithm: Finds an orthonormal basis $u=u_1 ... u_k$ given linearly independent set $v = v_1 ... v_k$ where:\n",
    "    * $u_1 = v_1$\n",
    "    * $u_2 = v_2 - \\frac{\\langle v_2, u_1 \\rangle}{\\langle u_1, u_1 \\rangle}$\n",
    "    * $u_3 = v_3 - \\frac{\\langle v_3, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} - \\frac{\\langle v_3, u_2 \\rangle}{\\langle u_2, u_2 \\rangle} $\n",
    "    * ...\n",
    "    * $u_k = v_k - \\sum_i^{k-1} \\frac{\\langle v_k, u_i \\rangle}{\\langle u_i, u_i \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms \n",
    "### Vector norms\n",
    "* Zero norm: $\\| x\\|_0$ is the number of non-zero elements in $x$\n",
    "    * Formally $|\\{i| x_i \\neq 0\\}|$\n",
    "* P-norm: $\\| x\\|_p = (\\sum_i^N |x_i|^p) ^{\\frac{1}{p}}$\n",
    "    * Ex Euclidean norm: $\\| x\\|_2 = (\\sum_i^N x_i^2) ^{\\frac{1}{2}}$\n",
    "    * Ex one norm $\\| x\\|_1 = (\\sum_i^N |x_i|)$ \n",
    "    \n",
    "### Matrix norms\n",
    "Given $M \\in R^{m\\times n}$, the i'th eigenvalue of $X$ is denoted $\\sigma_i$ or $\\sigma_i(X)$\n",
    "* Fröbenius: $\\| X\\|_F = (\\sum_i^m \\sum_j^n X_{ij}^2) ^{\\frac{1}{2}} = (\\sum_i^{min(m,n)} \\sigma_i^2) ^{\\frac{1}{2}}$\n",
    "* 1-Norm: $\\| X\\|_1 = (\\sum_{i,j}^{m,n} |x_{i,j}|)$\n",
    "* Euclidean norm: $\\| X\\|_2 = \\sigma_{max}(X)$\n",
    "* Spectral norm (p-norm): $\\| X\\|_p = max_{v \\neq 0} \\frac{ \\| Xv\\|_p }{\\| v\\|_p}$\n",
    "* Nuclear norm (star norm): $\\| X\\|_* = \\sum_i^{min(m,n)} \\sigma_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives\n",
    "\n",
    "### Vectors\n",
    "* $\\frac{\\partial}{\\partial x} (b^T x) = \\frac{\\partial}{\\partial x} (x^T b) = b$\n",
    "* $\\frac{\\partial}{\\partial x} (x^T x) = \\frac{\\partial}{\\partial x} (\\| x\\|_2^2)= 2x$\n",
    "* $\\frac{\\partial}{\\partial x} (x^T Ax) = (A^T A) x$ and if $A$ is symm then $=2Ax$\n",
    "* $\\frac{\\partial}{\\partial x} (b^T Ax) = A^T b$\n",
    "* $\\frac{\\partial}{\\partial x} (\\| x-b\\|_2) = \\frac{x-b}{\\|x-b\\|_2}$\n",
    "\n",
    "### Matrices\n",
    "* $\\frac{\\partial}{\\partial X} (c^T Xb) = bc^T$\n",
    "* $\\frac{\\partial}{\\partial X} (\\| X\\|_F^2) = 2X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and eigenvectors\n",
    "* $Ax = \\lambda x$\n",
    "* $A \\in R^{N\\times N}$: square matrix, $x$: column vector, $\\lambda$: scalar\n",
    "\n",
    "### Find eigenvalues\n",
    "The EV problem: Given a matrix $A$ solve the characteristic equation $\\lambda$ s.t. $det(A - \\lambda I) = 0$ which will result in some high degree polynomial, the eigenvalues are then the roots of this polynomial.\n",
    "\n",
    "### Find eigenvectors\n",
    "For each eigenvalue $\\lambda_i$ it holds that $A-\\lambda I)x_i = 0$, $x_i, \\lambda_i$ being the i'th eigenvector, eigenvalue pair. This is a linear system and can be solved by Gaussian elimination.\n",
    "\n",
    "Eigenvectors are not normalized to unit vectors, which is often desired - to fix this perform the following operation $\\tilde{x} = \\frac{x}{\\|x\\|_2}$\n",
    "\n",
    "### Eigen-decomposition\n",
    "* $A$ can be decomposed as $A = Q \\Lambda Q^T$ where $Q$ is an orthogonal matrix ($QQ^T = I$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory\n",
    "* Joint probability of variables $X$ and $Y$: $P(x) := Pr[X = x] := \\sum_{y \\in Y} p(x,y)$\n",
    "* Coniditional probability: $P(x|y) := Pr[X = x | Y = y] := \\frac{p(x,y)}{p(y)}$ where $P(y) > 0$\n",
    "* Necessary property of probability density: $\\forall y\\in Y: \\sum_{x \\in X} p(x|y) = 1$\n",
    "* Marginal probability, chain rule: $p(x,y) = p(x|y) p(y)$\n",
    "* Bayes Theorem using chain rule and conditional probability: $p(x|y) = \\frac{p(y|x) p(x)}{p(y)}$\n",
    "* Independence between stochastic variables: $p(y|x) = p(y)$ then $p(x|y) = p(x)$\n",
    "* Probability of a sequencee of IID obs: $p(x_1, x_2 ... x_N) = \\Pi_i^N p(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange Multipliers\n",
    "### Constrained Optimization\n",
    "We are given the following:\n",
    "* $f(x)$: The objective function to optimize.\n",
    "* $g_i(x) \\leq 0$ for $i=1...n$: Inequality constraint(s)\n",
    "* $h_j(x) = a_j^T x - b_j = 0$ for $j=1...m$: Equality constraints(s)\n",
    "\n",
    "We then formulate a langrangian \n",
    "$$L(x, \\lambda, v) := f(x) + \\sum_i^n \\lambda_i g_i(x) + \\sum_j^m v_j h_j(x)$$ \n",
    "\n",
    "which is then the constrained optimization objective.\n",
    "\n",
    "### Dual Function\n",
    "$$D(\\lambda, v) := inf_x L(x, \\lambda, v) \\in R$$\n",
    "\n",
    "Finding lower bound on optimum $f(x^*)$\n",
    "\n",
    "Lagrange dual problem:\n",
    "* Maximize $D$\n",
    "* Subject to $\\lambda \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex Optimization\n",
    "![](convex-set.png \"Convex illustration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Figure 1: Set is convex; it is not possible to draw a line segment which will lie outside the set.\n",
    "* Figure 2: Not convex, line illustrates this\n",
    "* Figure 3: Not convex, since not completely closed, i.e. 2 points on the closed edge can form a line segment which falls on the open circumference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function $f: R^D \\rightarrow R$ is convex iff for all points $x,y \\in dom(f)$ and for all $\\theta \\in [0, 1]$\n",
    "$$f(\\theta x + (1 − \\theta)y) ≤ \\theta f(x) + (1 − \\theta)f(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](convex-graph.png \"Convex illustration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epigraph\n",
    "Convexity can also be defined from the Epigraph of a function $epi(f)$ which is the set of points lying above the graph of $f$.\n",
    "\n",
    "A function $f$ is convex if $epi(f)$ is a convex set.\n",
    "\n",
    "* The graph of $f$: $\\{(x, f(x))\\quad  | \\quad x \\in dom(f) \\}$\n",
    "* Epigraph of $f$: $epi(f) = \\{(x, t) \\quad | \\quad x \\in dom(f), \\quad f(x) \\leq t \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common convex functions\n",
    "Linear functions: $f(x) = a^T x$\n",
    "\n",
    "Affine functions: $f(x) = a^T x + b$\n",
    "\n",
    "Exponentials: $f(x) = exp(\\alpha x)$\n",
    "\n",
    "Norms in $R^D$ are convex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with PCA\n",
    "\n",
    "Given $X \\in R^{D \\times N}$ (a dataset)\n",
    "* $D$: The dimension of each observation\n",
    "* $N$: The number of observations.\n",
    "\n",
    "PCA builds on SVD of the covariance matrix of a dataset $X$. The covariance matrix is a symmetrical matrix of dimension $D$ defining the features' covariance, $\\Sigma_X = cov[X_i, X_j]$ for all features. Since $\\Sigma_X$ is symmetrical of dimension $R^{D\\times D}$ the resulting decomposition $U D V^T$ has the property that $U = V$ and $U,V \\in R^{D \\times D}$.\n",
    "\n",
    "Goal: Reduce dimension $D$ to dimension $K$, s.t. $K << D$\n",
    "* I.e. transform $X \\rightarrow \\tilde{X} \\in R^{K \\times N}$\n",
    "\n",
    "## PCA (Principal Component Analysis)\n",
    "\n",
    "### PCA Procedure\n",
    "#### (1) Compute empirical mean observation\n",
    "Compute the mean along the rows: $\\bar{x} = \\frac{1}{N} \\sum_i^N x_i$, $\\bar{x} \\in R^D$\n",
    "\n",
    "#### (2) Center dataset \n",
    "Center wrt. the empirical mean, by subtracting the mean observation: $\\bar{X} = X - \\bar{x}$\n",
    "\n",
    "#### (3) Compute the covariance matrix\n",
    "Covariance matrix of $X$: $\\Sigma_X = \\frac{1}{N} \\sum_i^N (x_i - \\bar{x}) (x_i - \\bar{x})^T = \\frac{1}{N} \\bar{X} \\bar{X}^T$\n",
    "\n",
    "#### (4) Perform EV decomposition \n",
    "Decompose the the cov. matrix : $\\Sigma_X = U \\Lambda U^T$ (see chapter 1, Eigen-decomposition)\n",
    "* $U \\in R^{D\\times D}$, D is the outer dimension\n",
    "* $\\Lambda \\in R^{D \\times D}$ diagonal matrix (since cov matrix is symmetrical of dimension $D \\times D$)\n",
    "\n",
    "It then holds that $diag(\\Lambda) = \\sigma_i(\\Sigma_X)$ for $i=1...D$ in descending order.\n",
    "\n",
    "$U \\in R^{D\\times K}$\n",
    "Select the first $K < D$ for which a substantial amount of data is preserved, for this it can help inspecting the explained variance: $var = \\frac{\\sum_i^K \\sigma_i^2}{\\sum_j^D \\sigma_j^2}$. A good choice for $K$ will preserve more than 90% of data, while still being a much smaller than D.\n",
    "\n",
    "The $K$ first eigenvectors are then found as the $K$ first columns in $U$, that is, $U_K = [u_1 ... u_K]$ and $\\sigma = [\\sigma_1 ... \\sigma_K]$\n",
    "    \n",
    "#### (5) Compressing the Dataset\n",
    "Downproject centered dataset to new basis: $\\bar{Z} = U_K^T \\bar{X}$\n",
    "\n",
    "#### (6) Reconstructing the Dataset \n",
    "To reconstruct dataset: \n",
    "* Up-project to original basis of dimension $D$: $\\tilde{\\bar{X}}_K = U_K \\bar{Z} = U_K^T U_K \\bar{X}$\n",
    "    * Here, we used the orthonormality of $U$, i.e. $U U^T = I$\n",
    "* Undo centering by adding the mean observation $\\bar{x}$ once again: $\\tilde{X}_K = \\tilde{\\bar{X}}_K + \\bar{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "![](svd.png \"SVD illustration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above illustration, and denoting $Sigma$ as $D$ instead, SVD builds on the following:\n",
    "\n",
    "Given a matrix (ex dataset) $X \\in R^{N \\times M}$, $X$ can be decomposed as\n",
    "* $X = U D V^T$\n",
    "\n",
    "* $U^TU = V^T V = I$\n",
    "* Columns of $U$: Eigenvectors of $X X^T$\n",
    "* Columns of $V$: Eigenvectors of $X^T X$\n",
    "* Diag(D): $\\sqrt{\\sigma(X X^T)}$ (also called **singular values**)\n",
    "    * Note: $\\sigma(X X^T) = \\sigma(X^T X)$\n",
    "* $U \\in R^{N\\times N}, D \\in R^{N \\times M}, V \\in R^{M\\times M}$\n",
    "    * We do not require $N = M$, but if $N = M$ then $ U = V$ and $U, D, V \\in R^{N \\times N}$\n",
    "    \n",
    "### The SVD procedure\n",
    "\n",
    "#### (1) Compute Eigenvalues\n",
    "Compute the eigenvalues $\\lambda = [\\lambda_1 ... \\lambda_K]$ for $X X^T$, the singular values are then $\\sigma_i(X) = \\sqrt{\\lambda_i(X)}$.\n",
    "\n",
    "Find the eigenvalues with the characteristic equation: $det(X - \\lambda I) = 0$\n",
    "\n",
    "#### (2) Compute V\n",
    "Solve the linear system: $(X^T X) v_i = \\lambda_i v_i $ for all $\\lambda_i$. The vector $v_i$ is then the eigenvector for eigenvalue $\\lambda_i$.\n",
    "* On matrix form: $(X^T X) V = (D^T D) V$\n",
    "* $D^T D$ being the diagonal matrix containing eigenvalues for $X^T X$.\n",
    "The right singular vectors $V = [v_1 ... v_K]$ are now identified.\n",
    "\n",
    "#### (3) Compute U\n",
    "Solve for $X v_i = \\sigma_i u_i$, or on matrix form $X V = D U$ where $U = [u_1 ... u_K]$ are the left singular vectors. Concretely, this is solved as:\n",
    "* $D^{-1} X V = U D D^{-1} =  X V D^{-1}= U$\n",
    "\n",
    "#### (4) Normalization\n",
    "The columns of $U$ and $V$ should be normalized in order to be unit vectors.\n",
    "\n",
    "#### (5) Reconstruction\n",
    "From $X V D^{-1}= U$ let us reconstruct $X$:\n",
    "* $X V D^{-1} D = U D$ so now $X V = U D$\n",
    "* $X V V^T = U D V^T$, and since $V V^T = I$, this means means $X = U D V^T$.\n",
    "\n",
    "### Low-Rank Reconstruction\n",
    "The dataset can be compressed and reconstructed using the first $M$ singular values of $D$, the first $M$ columns of $U$ and first $M$ rows of $V^T$.\n",
    "* Concretely: $\\tilde{X}_M = U_M D_M V_M^T$\n",
    "\n",
    "### Eckart-Young Theorem\n",
    "Reconstruction error: Frobenius norm or Euclidean\n",
    "* Frobenius: $\\|X - \\tilde{X}_M\\|_F = (\\sum_{i=K}^M \\sigma_i^2)^{1/2} = (\\sum_{i=K}^M \\lambda_i)^{1/2}$\n",
    "* Euclidean: $\\|X - \\tilde{X}_M\\|_2 = \\sigma_{K+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Approximation & Reconstruction\n",
    "\n",
    "Core objective: Let reconstructed matrix be $B$ and $rank(B) = k$, and the optimal reconstruction with the minimal error is $A_k$.\n",
    "\n",
    "$$A_k = argmin_{k} \\|A - B\\|_F^2 = argmin_{k} [\\sum_{i,j \\in I} (a_{ij} - b_{ij})^2]$$\n",
    "$I$: Observed indices\n",
    "\n",
    "This is important because unobserved entries are _missing_ not just 0. Therefore we define a weighted Frobenius norm which uses a mask $G \\in \\{0,1\\} ^{m\\times n}$ which defines whether or not an element is observed or not.\n",
    "\n",
    "$$\\|X\\|_G := (\\sum_{i,j} g_{ij} x_{ij}^2)^{\\frac{1}{2}}$$\n",
    "\n",
    "The objective is thus:\n",
    "$$B^* = \\|A - B\\|_G^2$$ such that $rank(B) \\leq k$\n",
    "\n",
    "\n",
    "Problem: Not a convex optimization problem!\n",
    "\n",
    "Let $B \\in R^{m\\times n}$ and the factorization of $B$ be $B = UV$ s.t.\n",
    "* $U \\in R^{m \\times k}$\n",
    "* $V \\in R^{k \\times n}$\n",
    "* $rank(B) \\leq k$\n",
    "\n",
    "New objective: $$f(U,V) = \\frac{1}{|I|} \\sum_{i,j \\in I} (a_{ij} - \\langle u_i, v_j \\rangle ^2)$$\n",
    "\n",
    "### Alternating Least Squares\n",
    "Idea: Optimize $f$ wrt. $U$ and $V$ in an alternating fashion. When $U$ is optimized, $V$ is frozen and vice versa.\n",
    "\n",
    "Convex objectives:\n",
    "* $f_1 = U \\leftarrow argmin_U f(U, V)$\n",
    "* $f_2 = V \\leftarrow argmin_V f(U, V)$\n",
    "\n",
    "Optimize $f_1$ and then $f_2$ repeatedly until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "Unsupervised learning algorithm, finds optimal clustering based on a distance measure between points, ex Euclidean Norm.\n",
    "\n",
    "Given:\n",
    "* Dataset $X \\in R^{D \\times N}$ (D: dimensionality, N: number of obs)\n",
    "* Number of desired clusters: $K$\n",
    "* Assignment matrix: $Z \\in \\{0,1\\}^{N \\times K}$\n",
    "* Centroid matrix: $U \\in R^{D\\times K}$\n",
    "\n",
    "Objective function:\n",
    "* $min_{U,Z} J(U,Z) = \\|X - UZ^T\\|_F^2 = \\sum_n^N \\sum_k^K z_{n,k} \\cdot \\|x_n - u_k\\|_2^2$\n",
    "* Here we used that $Z$ is a binary matrix, and therefore it can pulled out of the norm (which makes things easier).\n",
    "\n",
    "Procedure/Algorithm:\n",
    "1. Initialize \n",
    "    * Assign each vector $[u_1 ... u_K]$ to a datapoint in $X$ such that no centroids are the same\n",
    "2. Cluster assignment\n",
    "    * Assign each datapoint $x_n$ to a cluster using the nearest centroid $k^*(x_n) = argmin_k \\|x_n - u_k\\|_2$\n",
    "    * Update $Z$ with the assignment: $Z_{n, k^*} = 1$ and $Z_{n, k \\neq k^*} = 0$\n",
    "3. Update centroids\n",
    "    * $u_k = \\frac{\\sum_n^N z_{k,n} x_n}{\\sum_n^N z_{k,n}}$\n",
    "    * This is the average coordinate of all points $x_n$ assigned to cluster $k$.\n",
    "\n",
    "Repeat step 2 and 3 until convergence which happens when assignments stop changing, i.e. $Z_{old} = Z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "Probability based model - which words occurs together with which probability.\n",
    "\n",
    "Log-likelihood: $L(\\theta; w) = \\sum_{t=1}^{T} \\sum_{\\Delta \\in I} log p_\\theta (w^{t+\\Delta} | w^t)$\n",
    "* $w = [w_1, ... ,w_T]$: A sequence of words\n",
    "* $I = [-R,...,-1, 1,...,R]$: A window of offsets\n",
    "\n",
    "Latent Vector Model: $w \\rightarrow (x_w, b_w) \\in R^{D+1}$\n",
    "* $x_w$: Embedding of $w$\n",
    "* $b_w$: Bias\n",
    "\n",
    "Log-bilinear model: $log p_\\theta (w|w') = \\langle x_w, x_{w'} \\rangle + b_w$\n",
    "\n",
    "Softmax of bi-linear model: $p_\\theta (w|w') = Pr[w \\text{ occurs close to } w'] = p_\\theta (w|w') = \\frac{log p_\\theta (w|w')}{Z_\\theta(w')}$\n",
    "* Where the latent variables $Z_\\theta(w') = \\sum_{v \\in V} exp(\\langle x_w, x_{w'} \\rangle + b_w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust PCA\n",
    "* Corrupted points\n",
    "\n",
    "Convex optimization\n",
    "* Convex sets\n",
    "* Rank of matrix\n",
    "* Norms and zero norm\n",
    "\n",
    "Lagrangians\n",
    "* Lagrange duality\n",
    "* Lagrangian Dual function\n",
    "\n",
    "Gradient descent\n",
    "* Condition for learning rate to guarantee convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
